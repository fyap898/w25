{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zElamOWIvAP"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "Selections from the [Chapter 16](https://github.com/ageron/handson-ml3/blob/main/16_nlp_with_rnns_and_attention.ipynb) notebook from the Scikit-learn book. Much like the textbook author, I gave up trying to make it work with Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect google drive for persistence\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "model_root = Path(\"/content/drive/MyDrive/SavedModels/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67ZsihoA5PXc",
        "outputId": "d854944e-f06d-46b4-bffb-60ab541d5b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if IS_COLAB:\n",
        "    import os\n",
        "    os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "    import tf_keras\n",
        "\n",
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ],
      "metadata": {
        "id": "V-DKzyCVivHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Piq5se2pKzx"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
        "              \"accelerator.\")\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll continue on with the English-Spanish translation task, so let's re-download and prepare the data."
      ],
      "metadata": {
        "id": "MxlV7_olJXd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD7ca5TxIvAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f706964e-fb44-4c2d-88ab-fcf9fc198b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "# Even loading the data has to change for keras 2/3\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhacerWLIvAc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsaDIKEjIvAc"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "batch_size = 32\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length=max_length,\n",
        "    pad_to_max_tokens=True,\n",
        ")\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length=max_length,\n",
        "    pad_to_max_tokens=True,\n",
        ")\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHHvqSgNIvAc"
      },
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLi1ozzswYU8"
      },
      "source": [
        "## RNN with Attention\n",
        "We'll define a bidirectional LSTM model, but this time add an `Attention` layer.\n",
        "\n",
        "The following cell is the same as in the vanilla LSTM examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk1dqErZwYU8"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "\n",
        "# fairly arbitrary size for the word embeddings\n",
        "# You could probably sub in pre-trained embeddings here\n",
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
        "\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.keras.layers.Concatenate(axis=-1)([encoder_state[0], encoder_state[2]]),  # short-term\n",
        "                 tf.keras.layers.Concatenate(axis=-1)([encoder_state[1], encoder_state[3]])]  # long-term\n",
        "\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhyfF526wYU8"
      },
      "source": [
        "Instead of just connecting the encoder and decoder directly, we'll put an `Attention` layer in the middle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvP4MT5mwYU8"
      },
      "outputs": [],
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "# query = decoder_outputs, value = encoder_outputs\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trying to figure out shape mismatch error, no luck\n",
        "for tensor in [decoder_outputs, encoder_outputs, attention_outputs]:\n",
        "    print(tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d024TF_9MjQ",
        "outputId": "0c8cadaf-481f-44af-98a3-d6824a64ac51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 50, 512)\n",
            "(None, 50, 512)\n",
            "(None, 50, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GPXEHIzwYU8"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI93LZEQwYU9",
        "outputId": "98941955-58e8-4a7c-b1c0-d590138d60a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " text_vectorization (TextVe  (None, 50)                   0         ['input_1[0][0]']             \n",
            " ctorization)                                                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 50, 128)              128000    ['text_vectorization[0][0]']  \n",
            "                                                                                                  \n",
            " text_vectorization_1 (Text  (None, 50)                   0         ['input_2[0][0]']             \n",
            " Vectorization)                                                                                   \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  [(None, 50, 512),            788480    ['embedding[0][0]']           \n",
            " al)                          (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 50, 128)              128000    ['text_vectorization_1[0][0]']\n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 512)                  0         ['bidirectional[0][1]',       \n",
            "                                                                     'bidirectional[0][3]']       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 512)                  0         ['bidirectional[0][2]',       \n",
            " )                                                                   'bidirectional[0][4]']       \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 50, 512)              1312768   ['embedding_1[0][0]',         \n",
            "                                                                     'concatenate[0][0]',         \n",
            "                                                                     'concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 50, 512)              0         ['lstm_1[0][0]',              \n",
            "                                                                     'bidirectional[0][0]']       \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 50, 1000)             513000    ['attention[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2870248 (10.95 MB)\n",
            "Trainable params: 2870248 (10.95 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "attn_path = model_root / \"attention_model.keras\"\n",
        "if attn_path.exists():\n",
        "    attention_model = tf.keras.models.load_model(attn_path)\n",
        "else:\n",
        "    attention_model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
        "    attention_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "                metrics=[\"accuracy\"])\n",
        "    attention_model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "            validation_data=((X_valid, X_valid_dec), Y_valid))\n",
        "    attention_model.save(attn_path)\n",
        "\n",
        "attention_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode/decode one word at a time until we predict endofseq\n",
        "def translate(model, sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = tf.constant([sentence_en])  # encoder input\n",
        "        X_dec = tf.constant([\"startofseq \" + translation])  # decoder input\n",
        "        y_proba = model.predict((X, X_dec), verbose=False)[0, word_idx]  # last token's probas\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "        translation += \" \" + predicted_word\n",
        "    return translation.strip()"
      ],
      "metadata": {
        "id": "Pku5s_FS5sIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(attention_model, \"Is class time over yet?\")"
      ],
      "metadata": {
        "id": "u9gyyJCoU_Sz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7e44ca8-b78b-4e5a-ff5d-997a43c43223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ya hace clases hora'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rranxGjBwYU9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88cb45ec-d2a3-4662-ddf9-27ceee214584"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me encanta ir a la playa hace [UNK] dónde puedo encontrar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "translate(attention_model, \"I love to go to the beach do you know where I can find it?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_model.evaluate((X_valid, X_valid_dec), Y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F3j5zoa8-Bx",
        "outputId": "32608d1a-ab3b-4b9a-e20c-f2620e143426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "593/593 [==============================] - 21s 18ms/step - loss: 1.3185 - accuracy: 0.7055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.318494439125061, 0.7055259346961975]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyfXqNwqIvAe"
      },
      "source": [
        "## Attention Is All You Need: The Transformer Architecture\n",
        "### Positional encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGlEST53IvAe"
      },
      "outputs": [],
      "source": [
        "max_length = 50  # max length in the whole training set\n",
        "embed_size = 128\n",
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "batch_max_len_enc = encoder_embeddings.shape[1]\n",
        "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
        "batch_max_len_dec = decoder_embeddings.shape[1]\n",
        "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KScTun-gIvAe"
      },
      "source": [
        "Alternatively, we can use fixed, non-trainable positional encodings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0uLRebyIvAe"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length),\n",
        "                           2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYBPHveYIvAe"
      },
      "outputs": [],
      "source": [
        "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
        "encoder_in = pos_embed_layer(encoder_embeddings)\n",
        "decoder_in = pos_embed_layer(decoder_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcGD4tCcIvAf"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90c_ak_yIvAf"
      },
      "outputs": [],
      "source": [
        "N = 2  # instead of 6\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = 128  # for the first Dense layer in each Feed Forward block\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
        "Z = encoder_in\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSRJgBpvIvAf"
      },
      "outputs": [],
      "source": [
        "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
        "causal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\n",
        "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-LRRGRmIvAf"
      },
      "outputs": [],
      "source": [
        "encoder_outputs = Z  # let's save the encoder's final outputs\n",
        "Z = decoder_in  # the decoder starts with its own inputs\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGc743OPIvAf"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly 2 or 3 hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFKsPvB5IvAf",
        "outputId": "20ddc5a2-a178-4e97-ba80-3a6b2b29cead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " text_vectorization_1 (Text  (None, 50)                   0         ['input_2[0][0]']             \n",
            " Vectorization)                                                                                   \n",
            "                                                                                                  \n",
            " text_vectorization (TextVe  (None, 50)                   0         ['input_1[0][0]']             \n",
            " ctorization)                                                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 50, 128)              128000    ['text_vectorization_1[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.not_equal (TFOpLam  (None, 50)                   0         ['text_vectorization[0][0]']  \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 50, 128)              128000    ['text_vectorization[0][0]']  \n",
            "                                                                                                  \n",
            " positional_encoding (Posit  (None, 50, 128)              0         ['embedding[0][0]',           \n",
            " ionalEncoding)                                                      'embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 1, 50)                0         ['tf.math.not_equal[0][0]']   \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 50, 128)              527488    ['positional_encoding[0][0]', \n",
            " iHeadAttention)                                                     'tf.__operators__.getitem[0][\n",
            "                                                                    0]',                          \n",
            "                                                                     'positional_encoding[0][0]'] \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 50, 128)              0         ['multi_head_attention[0][0]',\n",
            "                                                                     'positional_encoding[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 50, 128)              256       ['add[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 50, 128)              16512     ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 50, 128)              16512     ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 50, 128)              0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 50, 128)              0         ['dropout[0][0]',             \n",
            "                                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 50, 128)              256       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 50, 128)              527488    ['layer_normalization_1[0][0]'\n",
            " ltiHeadAttention)                                                  , 'tf.__operators__.getitem[0]\n",
            "                                                                    [0]',                         \n",
            "                                                                     'layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 50, 128)              0         ['multi_head_attention_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.math.not_equal_1 (TFOpL  (None, 50)                   0         ['text_vectorization_1[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 50, 128)              256       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 1, 50)                0         ['tf.math.not_equal_1[0][0]'] \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 50, 128)              16512     ['layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.math.logical_and (TFOpL  (None, 50, 50)               0         ['tf.__operators__.getitem_1[0\n",
            " ambda)                                                             ][0]']                        \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 50, 128)              16512     ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 50, 128)              527488    ['positional_encoding[1][0]', \n",
            " ltiHeadAttention)                                                   'tf.math.logical_and[0][0]', \n",
            "                                                                     'positional_encoding[1][0]'] \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 50, 128)              0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 50, 128)              0         ['multi_head_attention_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'positional_encoding[1][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 50, 128)              0         ['dropout_1[0][0]',           \n",
            "                                                                     'layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 50, 128)              256       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 50, 128)              256       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 50, 128)              527488    ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'tf.__operators__.getitem[0]\n",
            "                                                                    [0]',                         \n",
            "                                                                     'layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 50, 128)              0         ['multi_head_attention_3[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 50, 128)              256       ['add_5[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 50, 128)              16512     ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 50, 128)              16512     ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 50, 128)              0         ['dense_6[0][0]',             \n",
            "                                                                     'layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 50, 128)              256       ['add_6[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.math.logical_and_1 (TFO  (None, 50, 50)               0         ['tf.__operators__.getitem_1[0\n",
            " pLambda)                                                           ][0]']                        \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 50, 128)              527488    ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'tf.math.logical_and_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 50, 128)              0         ['multi_head_attention_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 50, 128)              256       ['add_7[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (Mu  (None, 50, 128)              527488    ['layer_normalization_7[0][0]'\n",
            " ltiHeadAttention)                                                  , 'tf.__operators__.getitem[0]\n",
            "                                                                    [0]',                         \n",
            "                                                                     'layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 50, 128)              0         ['multi_head_attention_5[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 50, 128)              256       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 50, 128)              16512     ['layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 50, 128)              16512     ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 50, 128)              0         ['dense_8[0][0]',             \n",
            "                                                                     'layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 50, 128)              256       ['add_9[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 50, 1000)             129000    ['layer_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3684584 (14.06 MB)\n",
            "Trainable params: 3684584 (14.06 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer_path = model_root / \"transformer.keras\"\n",
        "if transformer_path.exists():\n",
        "    transformer_model = tf.keras.models.load_model(transformer_path, custom_objects={'PositionalEncoding': PositionalEncoding})\n",
        "else:\n",
        "    Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
        "    transformer_model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                        outputs=[Y_proba])\n",
        "    transformer_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "                metrics=[\"accuracy\"])\n",
        "    transformer_model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "            validation_data=((X_valid, X_valid_dec), Y_valid))\n",
        "    transformer_model.save(transformer_path)\n",
        "\n",
        "transformer_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4cYeTqTIvAf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3f7d5a8-2a3c-4c59-cea2-485c9711c284"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol y iba a la playa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "translate(transformer_model, \"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model.evaluate((X_valid, X_valid_dec), Y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j31Blize9I6Z",
        "outputId": "248ac1b4-6400-4641-f223-8e01bd8c32e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "593/593 [==============================] - 36s 46ms/step - loss: 1.0695 - accuracy: 0.7320\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0695241689682007, 0.7320123910903931]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
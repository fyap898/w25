{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Polynomial Regression\n",
    "There's some fake data in the file \"data.csv\", with a single feature (x) and a true value (y). Your task is to:\n",
    "1. Load the data and look at it\n",
    "2. Split it into training, validation, and test sets\n",
    "3. Create your design matrix\n",
    "4. Implement gradient descent to find the best fit polynomial\n",
    "5. Evaluate your model's performance and experiment with different hyperparameters\n",
    "\n",
    "It's up to you to decide what degree polynomial to fit the data, and you can also play around with stochastic gradient descent, mini-batch, hyperparameters, etc.\n",
    "\n",
    "Try to do this without the use of scikit learn or other libraries aside from `numpy` and `matplotlib`.\n",
    "\n",
    "## Step 0: Import libraries and seed your random number generator\n",
    "It's usually a good idea to start with a consistent random number seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#TODO: random number seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data and look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.loadtxt(\"data.csv\", delimiter=\",\", skiprows=1, unpack=True)\n",
    "#TODO: visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split the data\n",
    "If you're selecting random samples, I'd recommend a random seed to have consistent results and avoid data leakage when you re-run the notebook.\n",
    "\n",
    "Weird numpy quirk: by default, a 1D array has a shape of `(n,)`, but to behave as a proper vector, we need to convert it to be `(n, 1)`. An easy way to do this is to pass `np.newaxis` as the second index when sampling your `y` data, e.g.:\n",
    "\n",
    "```python\n",
    "y_train = y[<selected range>, np.newaxis]\n",
    "```\n",
    "\n",
    "Don't worry about the x values for now, as we'll be matrixifying them shortly anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create your **design matrix** $X$.\n",
    "\n",
    "For the example given in class, the design matrix was simply a column of 1s concatenated with the feature vector, i.e.:\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_m \\end{bmatrix}$$\n",
    "\n",
    "For this exercise, you probably want to fit a higher degree polynomial, so the design matrix will be something like:\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\ldots & x_1^d \\\\ 1 & x_2 & x_2^2 & \\ldots & x_2^d \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_m & x_m^2 & \\ldots & x_m^d \\end{bmatrix}$$\n",
    "\n",
    "where $d$ is the degree of the polynomial you want to fit. Try multiple degrees and see what gives the best results.\n",
    "\n",
    "> A note on scaling: the range of x values in this example is fairly small, but if you choose a high degree polynomial you will still end up with fairly different scales for your \"features\". Consider normalizing each column of the design matrix (other than the first column accounting for the bias term), remembering to calculate your scaling parameters on the training data and apply them to the validation/test data.\n",
    "\n",
    "Since you've got three x vectors (train/test/val), you might want to define a function to create the design matrix given a vector x and a degree d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement gradient descent\n",
    "This has a number of sub components. First you'll need to define your gradient function. For mean squared error, the gradient can be calculated as:\n",
    "\n",
    "$$\\nabla_{\\theta} MSE = \\frac{2}{m}X^T(X\\mathbf{\\theta} - \\mathbf{y})$$\n",
    "\n",
    "where $X$ is your design matrix, $\\mathbf{\\theta}$ is the current parameter vector, and $\\mathbf{y}$ is the true target value.\n",
    "\n",
    "It'll also be useful to define the actual mean squared error to evaluate your model:\n",
    "\n",
    "$$MSE = \\frac{1}{m}(\\mathbf{X} \\mathbf{\\theta} - \\mathbf{y})^T (\\mathbf{X} \\mathbf{\\theta} - \\mathbf{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can define your hyperparameters and run your gradient descent. For minibatch gradient descent, you'll need to define:\n",
    "- learning rate $\\eta$ (usually in the range of $10^{-5}$ to $10^{-2}$)\n",
    "- number of epochs\n",
    "- batch size\n",
    "\n",
    "Recall that an **epoch** is a full pass through your training data - or rather, sampling your training data $m_{\\mathrm{train}}$ times, as you are not guaranteed to select every sample in every epoch. This means that for every epoch, you'll need to select $\\frac{m_{\\mathrm{train}}}{b}$ batches, where $b$ is your batch size.\n",
    "\n",
    "The general algorithm for gradient descent is:\n",
    "1. Start with a random $\\mathbf{\\theta}$\n",
    "2. Pick a random batch of instances $\\mathbf{x}_i$ (rows in your design matrix)\n",
    "3. Calculate the gradient $\\nabla_{\\mathbf{\\theta}}$ for the current $\\mathbf{\\theta}$ and $\\mathbf{x}_i$\n",
    "4. Update $\\mathbf{\\theta}$ as $\\mathbf{\\theta} = \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}}$\n",
    "5. Repeat 2-4 until some stopping criterion is met\n",
    "\n",
    "Stochastic gradient descent is when your batch size is 1, while standard gradient descent is a batch size of $m_{\\mathrm{train}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate your model's performance and experiment\n",
    "Now that you've computed a $\\mathbf{\\theta}$, apply it to your validation set to see how well your model performs, perhaps by plotting the data as well as the best fit curve. You could also declare an array of mse values to update at each epoch to plot and compare the performance of train and validation sets.\n",
    "\n",
    "Try changing various hyperparameters, like $\\eta$, batch size, number of epochs, etc. You can also experiment with the degree of the polynomial, keeping in mind that dropping higher order terms is a form of regularization. If you didn't rescale your design matrix earlier, try it now! Once you're happy with the final model, see how it behaves on your held out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

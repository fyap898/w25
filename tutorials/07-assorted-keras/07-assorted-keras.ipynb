{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Keras \n",
    "Based on Aurélien Geron's [Chapter 10 Notebook](https://github.com/ageron/handson-ml3/blob/main/10_neural_nets_with_keras.ipynb).\n",
    "\n",
    "## Objectives\n",
    "- Continue the basics of neural networks with Keras\n",
    "- Build and train a simple regression model\n",
    "- Explore some more features of the Keras/Tensorflow ecosystem\n",
    "\n",
    "You may want to run this on Colab, but it isn't using a particularly large dataset so locally should work as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic model: predicting house prices\n",
    "And you thought you were done with the California housing dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – load and split the California housing dataset, like earlier\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "# split it again to get a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "print(\"Training instances: \", y_train.shape)\n",
    "print(\"Validation instances: \", y_val.shape)\n",
    "print(\"Testing instances: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's use the approach of a constant number of neurons per layer and see how things go.\n",
    "\n",
    "The [Adam optimizer](https://arxiv.org/abs/1412.6980) is a very popular adaptive learning rate method that takes into account both the first and second moments (mean and variance) of the gradients. The step ends up being with high moments, resulting in smaller steps when the gradient is both small and smooth.\n",
    "\n",
    "We'll also use a `Normalization` layer to scale the input features (the `StandardScaler` from scikit-learn would also work).\n",
    "\n",
    "### Exercise 1\n",
    "Modify the cell below to have 3 fully connected layers with 50 neurons each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers # just to make the names less unwieldy\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # input shape does not include batch size\n",
    "    layers.Input(X_train.shape[1:]), \n",
    "    layers.Normalization(name=\"norm\"),\n",
    "    # TODO: Add 3x fully connected layers with 50 neurons each and relu activation\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "# the adapt method computes the mean and standard deviation of the input features\n",
    "# Note that only the training data is used to compute the mean and standard deviation!\n",
    "model.get_layer(\"norm\").adapt(X_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and plot the training curves\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_batch_size=len(X_val),\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation behaviour looks awfully weird. What might be happening?\n",
    "\n",
    "Take a peek at the [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) method docs to try to understand the training process (and hyperparameters) better. In particular, pay attention to the **batch size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try looking at the whole validation dataset\n",
    "mse_val, rmse_val = model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that a good RMSE value? Let's try a scatter plot to see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_val, model.predict(X_val), alpha=0.1)\n",
    "plt.xlabel(\"Actual House price ($100,000)\")\n",
    "plt.ylabel(\"Predicted House price ($100,000)\")\n",
    "# Probably better than our random forest model, but still not great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide & Deep neural network (see [paper](https://research.google/pubs/wide-deep-learning-for-recommender-systems/)) connects all or part of the inputs directly to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – reset the name counters and make the code reproducible\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Using the following code as a starting point, build a model with the following architecture:\n",
    "\n",
    "- Input layer, same as before but we need to be more explicit about it (i.e. specify the shape)\n",
    "- Normalization (same as before)\n",
    "- 2x Dense layers with only 30 neurons each this time and relu activation\n",
    "- Concatenation of the input and the output of the second Dense layer - also called a \"skip connection\"\n",
    "- Our output layer, same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=X_train.shape[1:])\n",
    "normalized = layers.Normalization(name=\"norm\")(input_)\n",
    "# TODO: 2x hidden layers\n",
    "concat = layers.Concatenate()([normalized, hidden_output])\n",
    "output = layers.Dense(1)(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "# don't forget to calculate the mean/std of your normalization layer\n",
    "model.get_layer(\"norm\").adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
    "mse_val = model.evaluate(X_val, y_val)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_val, rmse_val = model.evaluate(X_val, y_val)\n",
    "# plot the training curves\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is a bit worse than before, but we have far fewer parameters. However, the functional API allows for a lot more flexibility - the [original notebook](https://github.com/ageron/handson-ml3/blob/main/10_neural_nets_with_keras.ipynb) and associated text in chapter 10 goes into a lot more detail.\n",
    "Finally, you can also define a model by subclassing the `Model` class and defining your own `call` method to create a more dynamic model. This is also the PyTorch way of doing things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring a Model\n",
    "Ultimately after spending all this time training a model, you'll probably want to save the weights so you can use it later. You can also define a **custom callback** to save the model periodically during training in case of a crash, timeout, to save the best intermediate result, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.keras\")\n",
    "\n",
    "# To load it again:\n",
    "# model = tf.keras.models.load_model(\"model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Here we'll define two simple callbacks (built in to Keras): One for early stopping and one for saving the model at the end of each epoch. The early stopping callback will stop the training if the validation loss stops decreasing for a certain number of epochs.\n",
    "\n",
    "We can also define custom callbacks - again the original notebook goes into a lot more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# make a copy of the model, with the same architecture, but randomly initialized weights\n",
    "model = tf.keras.models.clone_model(model)\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# TODO: define a checkpoint_cb using tf.keras.callbacks.ModelCheckpoint\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), callbacks=[checkpoint_cb, early_stopping_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model performance\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Let's use the Keras Tuner to do a quick hyperparameter search on the housing price prediction problem. For this to work, we need to wrap our model creation into a function that takes an `hp` argument (for hyperparameters) and returns a model.\n",
    "\n",
    "We'll go back to the sequential model for simplicity - it was actually working the best anyway.\n",
    "\n",
    "### Exercise 4\n",
    "In the code below, try modifying the n_neurons parameter in the function generator to be a tunable hyperparameter. Then, run the random search to figure out the best hyperparameters for the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    # Original model had 3 hidden layers with 50 neurons each\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
    "    n_neurons = 50\n",
    "    #TODO: Modify n_neurons to be a tunable hyperparameter\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=X_train.shape[1:]))\n",
    "    model.add(layers.Normalization(name=\"norm\"))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    # adapt the normalization layer as usual\n",
    "    model.get_layer(\"norm\").adapt(X_train)\n",
    "\n",
    "    # add on the optimizer and compile\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=5,\n",
    "    overwrite=True,\n",
    "    directory=\"random_search\",\n",
    "    project_name=\"california_housing\",\n",
    "    seed=42)\n",
    "\n",
    "random_search_tuner.search(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "For this exercise, we'll need a new dataset. Let's try the CGI `rock_paper_scissors` dataset - it's pretty uniformly the same hand angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "# let's try this rock_paper_scissors dataset\n",
    "rps_train, info = tfds.load('rock_paper_scissors', split='train', shuffle_files=True, with_info=True)\n",
    "rps_val, info = tfds.load('rock_paper_scissors', split='test', shuffle_files=True, with_info=True)\n",
    "print(info)\n",
    "\n",
    "i = 1\n",
    "for sample in rps_train.take(6):\n",
    "    ax = plt.subplot(2, 3, i)\n",
    "    i += 1\n",
    "    img = sample[\"image\"] / 255\n",
    "    label = sample[\"label\"]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(info.features[\"label\"].int2str(label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Modify the following two cells to include random rotations in the augmentation layers, then add augmentation to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the preprocessing and augmentation layers\n",
    "# The downsampling and rescaling layers will remain active during inference, \n",
    "# but the augmentation (random flip, rotation, zoom, crop, etc) will be switched off.\n",
    "preprocess = tf.keras.Sequential([\n",
    "    layers.Input(shape=(300, 300, 3)),\n",
    "    layers.Resizing(112, 112),\n",
    "    layers.Rescaling(1./255),\n",
    "])\n",
    "\n",
    "augment = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    #TODO: Try a random rotation\n",
    "])\n",
    "\n",
    "# Example of applying the pipeline to the last image shown above\n",
    "# rerun this several times to see how the randomization changes\n",
    "batch = tf.expand_dims(sample[\"image\"], axis=0)\n",
    "augmented = augment(preprocess(batch))\n",
    "augmented = tf.squeeze(augmented, axis=0)\n",
    "# the whole batch/augment/squeeze thing is all about making sure the dimensions match\n",
    "\n",
    "plt.imshow(augmented)\n",
    "plt.title(\"Augmented Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    preprocess,\n",
    "    #TODO: Add your augmentation here\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason the tfds format is a dictionary, but we need a tuple of input, output to pass to model.fit\n",
    "def wrangle_tfds(sample):\n",
    "    return tf.expand_dims(sample[\"image\"], axis=0), tf.expand_dims(tf.one_hot(sample['label'], depth=3), axis=0)\n",
    "\n",
    "epochs=5\n",
    "history = model.fit(\n",
    "    rps_train.map(wrangle_tfds),\n",
    "    validation_data=rps_val.map(wrangle_tfds),\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the validation accuracy with flips/rotations applied\n",
    "def augment_and_wrangle(sample):\n",
    "    img, label = wrangle_tfds(sample)\n",
    "    return augment(img), label\n",
    "\n",
    "# Augment the validation dataset\n",
    "augmented_val_ds = rps_val.map(augment_and_wrangle)\n",
    "\n",
    "# Evaluate the model on the augmented validation dataset\n",
    "accuracy = model.evaluate(augmented_val_ds)\n",
    "print(f\"Accuracy on augmented validation dataset: {accuracy[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
